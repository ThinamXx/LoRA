{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"**LoRA Implementation**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "_ = torch.manual_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset:\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# load the mnist dataset:\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_data_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# define the device:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model to classify the mnist dataset:\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n",
    "        super(DemoModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the model:\n",
    "model = DemoModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:30<00:00, 195.25it/s, loss=0.243]\n"
     ]
    }
   ],
   "source": [
    "# train the network for 1 epoch:\n",
    "def train(train_data_loader, net, epochs=5, total_iterations_limit=None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    total_iterations = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        \n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "        \n",
    "        data_iterator = tqdm(train_data_loader, desc='Epoch {}'.format(epoch+1))\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1, 28*28)) \n",
    "            loss = cross_el(output, y)  \n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "# train the network for 1 epoch:\n",
    "train(train_data_loader, model, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear1.weight': tensor([[ 0.0201,  0.0409,  0.0572,  ...,  0.0585,  0.0407,  0.0263],\n",
      "        [ 0.0055,  0.0478,  0.0310,  ...,  0.0062,  0.0027,  0.0230],\n",
      "        [ 0.0319,  0.0066,  0.0430,  ...,  0.0170,  0.0270,  0.0062],\n",
      "        ...,\n",
      "        [ 0.0599,  0.0681,  0.0363,  ...,  0.0808,  0.0345,  0.0717],\n",
      "        [ 0.0146,  0.0251,  0.0674,  ...,  0.0696,  0.0343,  0.0500],\n",
      "        [-0.0124,  0.0121,  0.0030,  ..., -0.0288, -0.0194,  0.0263]],\n",
      "       device='cuda:0'), 'linear1.bias': tensor([-3.7218e-02, -3.5061e-02,  6.7275e-03, -2.9251e-02, -5.8652e-02,\n",
      "        -5.8553e-02, -3.6413e-02, -3.0102e-02, -4.4707e-02, -2.0931e-02,\n",
      "        -5.1178e-02, -3.6080e-02, -4.1683e-02, -2.2179e-02,  2.5836e-03,\n",
      "         3.1643e-03, -5.0738e-02,  7.7742e-03, -4.0873e-02, -4.8966e-02,\n",
      "        -3.8418e-02, -4.4142e-02, -4.9995e-03, -9.9650e-03, -4.2807e-02,\n",
      "        -5.8977e-02, -1.9777e-02, -1.5686e-02, -4.5655e-02, -1.4754e-02,\n",
      "        -3.4274e-02, -6.9317e-03, -2.0472e-02, -4.2401e-03, -1.7719e-02,\n",
      "        -4.2614e-02, -7.3488e-02, -1.6617e-02, -9.3553e-03,  1.2338e-02,\n",
      "        -2.7786e-02, -3.9697e-02, -8.7998e-02, -3.3460e-02, -1.4294e-02,\n",
      "        -5.7202e-02, -5.6041e-02, -4.3227e-02,  4.4676e-03, -7.9173e-02,\n",
      "         2.1625e-03,  2.9007e-03,  2.2517e-02, -4.4550e-02, -3.3046e-02,\n",
      "        -2.7263e-02, -5.9221e-02, -5.3280e-03, -2.5238e-02, -5.1838e-02,\n",
      "        -2.5388e-02, -3.0703e-02, -5.3973e-02, -3.8565e-02,  2.3589e-02,\n",
      "        -1.8525e-02, -2.2739e-02, -3.3022e-02,  1.3685e-02, -1.5563e-03,\n",
      "        -4.5039e-02, -8.0197e-02, -5.0490e-02, -2.8715e-02, -2.4270e-02,\n",
      "         3.6709e-04,  2.1427e-03, -2.5397e-02,  1.3180e-03, -5.8407e-02,\n",
      "        -5.9534e-04,  1.4197e-02, -2.7635e-03,  1.7271e-02, -5.3441e-02,\n",
      "        -1.7631e-02, -4.4462e-02, -2.7769e-02, -3.8561e-02,  5.9646e-03,\n",
      "        -1.8815e-02, -3.0850e-02, -2.1697e-02, -6.9518e-02, -1.1745e-02,\n",
      "        -4.3245e-02, -6.1757e-02, -5.8331e-02, -3.1434e-02, -1.5846e-02,\n",
      "        -1.9493e-02, -3.5763e-02, -3.1293e-02, -1.3130e-02, -6.8127e-02,\n",
      "        -1.4872e-03, -1.6638e-02, -1.9009e-02,  8.9211e-05,  1.3140e-02,\n",
      "        -6.8778e-03, -4.4682e-02, -3.7288e-02, -2.3136e-02, -6.0881e-02,\n",
      "        -6.8826e-02,  8.9918e-03, -5.4202e-02, -5.8102e-02, -6.3043e-02,\n",
      "         2.2571e-02, -1.8462e-02, -2.3425e-02,  7.6779e-03, -2.5048e-02,\n",
      "        -6.6471e-02, -1.2751e-02, -7.3998e-02, -7.4694e-03,  1.1723e-02,\n",
      "        -5.2659e-02, -1.9210e-02, -6.8409e-02, -2.2455e-02, -5.2399e-02,\n",
      "        -2.2463e-02, -3.8097e-02, -2.7630e-02, -1.8062e-02, -3.4621e-02,\n",
      "         3.3698e-02, -3.4860e-02, -6.7122e-02, -1.9856e-02, -5.5792e-02,\n",
      "        -8.4766e-03, -4.9124e-03, -7.1191e-03,  2.6621e-02, -5.7437e-02,\n",
      "        -6.6332e-02, -7.2083e-03, -1.6995e-02, -8.0178e-02, -3.9687e-02,\n",
      "        -1.2920e-02, -1.1622e-02, -3.4975e-02, -2.0608e-02, -6.6958e-02,\n",
      "        -2.7267e-02, -5.2051e-02, -3.4321e-02, -2.8246e-02, -2.4886e-02,\n",
      "        -4.2132e-02,  1.6839e-02, -3.7927e-02, -2.7028e-02, -3.2863e-02,\n",
      "        -1.2973e-02, -6.6784e-02, -2.0361e-02,  1.1310e-03, -2.3913e-03,\n",
      "        -2.7935e-02, -3.8527e-02, -5.5652e-02,  2.5456e-02, -1.0705e-02,\n",
      "        -2.5078e-02, -1.1117e-02, -4.3691e-02, -5.3895e-02, -3.3191e-02,\n",
      "        -1.4107e-02, -2.6962e-03, -7.9078e-02,  2.3730e-03, -4.5682e-02,\n",
      "        -4.2135e-02, -2.8776e-02, -1.4418e-02,  3.8393e-03, -3.1684e-02,\n",
      "        -3.0479e-02, -4.2147e-02, -8.2436e-03, -4.0627e-02, -2.8472e-02,\n",
      "        -6.7324e-02,  4.4367e-02, -2.7165e-02, -7.8686e-03, -3.5067e-02,\n",
      "        -3.5854e-02, -6.2834e-02,  4.7604e-03, -3.6395e-02, -2.9546e-02,\n",
      "         1.2802e-02, -3.9748e-03, -6.4125e-02, -1.6372e-02, -8.7434e-03,\n",
      "        -3.3919e-02,  1.1646e-02, -5.8953e-02,  1.4274e-02, -3.0377e-02,\n",
      "         1.2889e-02, -2.9245e-02, -3.9979e-02, -1.7062e-02, -2.7731e-02,\n",
      "         2.3364e-02, -1.9081e-02, -3.2441e-02,  2.1311e-03, -2.0215e-02,\n",
      "        -4.5003e-02,  1.4092e-02,  3.1007e-03,  2.3865e-02, -5.3583e-02,\n",
      "        -4.6420e-02, -4.2313e-02, -6.3969e-02, -5.2159e-02, -3.7169e-02,\n",
      "        -3.9696e-02, -5.4958e-02, -5.1169e-02, -5.6501e-02, -5.7986e-02,\n",
      "        -3.0994e-02, -1.7665e-02, -4.1774e-02, -2.9553e-02, -7.2398e-02,\n",
      "        -2.2233e-02, -5.1251e-02, -3.4410e-02, -8.4130e-03, -5.7720e-02,\n",
      "         2.6188e-03, -2.2472e-02, -7.4071e-02, -9.0140e-03, -6.5545e-02,\n",
      "        -9.1828e-02, -4.9229e-02, -8.6953e-02, -9.2803e-02, -6.6053e-02,\n",
      "        -2.3480e-02,  7.9920e-03,  1.7859e-03,  2.5333e-02, -1.2782e-02,\n",
      "        -1.7099e-02, -2.2464e-02, -1.1013e-02, -5.2470e-02, -2.0081e-02,\n",
      "        -1.1210e-02, -3.6200e-02,  9.1243e-03, -8.0932e-02, -9.7582e-03,\n",
      "         1.1368e-02, -7.7824e-02, -2.0391e-02,  1.5122e-02, -3.5460e-02,\n",
      "         4.5588e-03, -2.7379e-02, -3.9353e-02,  9.6274e-03, -5.8947e-02,\n",
      "        -8.8866e-03,  9.1886e-03,  6.7436e-03, -4.4030e-02, -4.1016e-02,\n",
      "         2.7569e-02,  2.0534e-02, -8.4189e-03,  3.2584e-02, -3.7273e-02,\n",
      "        -1.7698e-02,  3.0224e-04, -6.5637e-02, -9.4090e-03, -1.4289e-02,\n",
      "        -4.1159e-02,  3.0316e-05, -5.9319e-02, -7.9057e-02, -2.3912e-03,\n",
      "        -4.2867e-02, -4.8034e-02,  1.8902e-02, -2.2391e-02,  2.3133e-02,\n",
      "        -6.5198e-02, -4.2629e-02, -2.9609e-02, -3.9347e-02, -3.8059e-02,\n",
      "        -4.5269e-03, -2.4378e-02, -6.6456e-02, -1.0442e-02, -3.0897e-02,\n",
      "        -4.4991e-02,  1.3221e-02,  2.4313e-03, -6.5102e-03, -3.6291e-03,\n",
      "        -1.0490e-02, -1.6654e-02, -3.6188e-02, -3.8359e-03, -3.2714e-02,\n",
      "        -6.0475e-02, -2.4301e-02, -5.5014e-02, -1.0148e-02,  1.8988e-02,\n",
      "         4.1914e-03, -7.2610e-02, -6.8546e-02, -3.3691e-02, -5.1725e-03,\n",
      "        -9.9360e-02, -1.2858e-02, -3.1082e-02, -1.3657e-03, -5.1933e-02,\n",
      "        -3.3660e-02, -2.7679e-02, -4.3759e-02, -4.7120e-02, -1.2919e-02,\n",
      "        -3.4925e-03, -1.8672e-02, -1.1971e-02,  1.5695e-02, -4.4555e-02,\n",
      "        -4.1915e-02, -8.0207e-03, -3.0007e-02, -2.4245e-02, -2.4378e-02,\n",
      "        -7.0280e-02, -2.3824e-02, -6.0057e-02, -2.8526e-02, -6.2195e-02,\n",
      "        -3.8701e-02, -4.9209e-02, -7.5904e-02, -5.2495e-03, -5.6003e-02,\n",
      "        -1.7914e-02,  6.1399e-04, -4.7834e-02, -5.7109e-04,  1.3358e-02,\n",
      "         2.7384e-03, -2.8625e-02, -1.1083e-04, -9.3208e-03, -2.7300e-02,\n",
      "        -6.4374e-02, -3.5767e-02, -1.6014e-02,  1.2759e-03,  8.6470e-03,\n",
      "        -1.3727e-02, -4.1234e-02, -4.9724e-02, -4.0952e-02, -5.1617e-02,\n",
      "        -4.3565e-02, -2.3963e-02, -8.3785e-03, -3.5685e-02, -7.3742e-02,\n",
      "        -3.7931e-02, -2.1226e-02, -5.3746e-02, -6.6718e-02, -6.2190e-02,\n",
      "         1.4865e-03, -3.2960e-03, -4.6141e-03, -3.7350e-02,  5.3403e-03,\n",
      "        -3.2432e-02, -2.6475e-02, -1.4345e-02, -5.3370e-02, -7.2155e-03,\n",
      "        -6.0526e-02, -4.3498e-02, -5.5664e-02, -2.1045e-02, -3.5399e-02,\n",
      "        -2.8633e-02, -4.0368e-02, -5.4818e-02, -1.7309e-02, -1.2534e-02,\n",
      "        -8.5261e-03, -7.3338e-03, -6.5489e-02, -3.2549e-02, -5.2190e-02,\n",
      "        -2.0783e-02, -8.2105e-03, -3.1236e-02, -1.1597e-02,  1.3606e-02,\n",
      "        -4.0830e-02, -1.5190e-02, -4.7462e-02, -4.9828e-02, -5.9136e-02,\n",
      "        -6.0516e-02, -1.9711e-02, -3.8438e-02, -5.0050e-04, -3.9761e-02,\n",
      "        -2.0606e-02, -3.5045e-02, -2.7569e-02, -2.7878e-02, -4.5648e-02,\n",
      "        -5.6842e-02, -7.2795e-02, -3.7637e-02,  1.6618e-02, -3.1038e-02,\n",
      "        -1.7157e-02, -5.5397e-04,  1.3728e-02, -1.5514e-02, -2.7119e-02,\n",
      "        -3.6279e-02, -1.4788e-02, -7.1365e-02, -4.5206e-03, -2.8667e-02,\n",
      "        -6.7755e-02, -3.2336e-02,  9.4582e-03, -6.8084e-02,  1.8576e-02,\n",
      "        -4.8714e-02, -7.5546e-02, -5.6798e-03, -1.3638e-02,  5.5549e-03,\n",
      "         2.9006e-03, -6.6724e-02, -4.7026e-03, -4.7205e-02,  7.9740e-03,\n",
      "         2.0723e-02, -1.3775e-02, -3.9563e-02,  2.2244e-03, -1.7701e-02,\n",
      "         1.6529e-02, -7.7923e-03, -6.1700e-03, -8.7958e-03, -4.2010e-02,\n",
      "         1.4935e-03, -5.6636e-03, -1.6712e-02,  3.9334e-03, -3.0010e-02,\n",
      "        -1.9822e-02, -6.4238e-02, -5.4928e-02,  4.3290e-03, -1.5883e-02,\n",
      "        -3.9421e-02, -4.2630e-03, -7.5647e-02, -3.7187e-02, -3.1110e-02,\n",
      "        -3.9870e-03,  9.6771e-03, -1.5772e-02, -7.4108e-02, -7.5551e-02,\n",
      "        -5.8516e-02, -6.5035e-02, -2.3939e-03, -9.0726e-03, -2.8412e-02,\n",
      "        -8.6112e-02, -6.3757e-02, -2.0130e-02, -6.5211e-03, -3.5769e-02,\n",
      "        -5.5349e-02, -4.4592e-02, -4.1653e-02, -7.3541e-02, -8.9155e-02,\n",
      "        -4.9270e-02, -1.5219e-02,  2.1715e-02, -3.6112e-02, -4.8113e-02,\n",
      "         4.1782e-03,  1.7570e-02, -1.5891e-02, -2.0869e-02, -6.1907e-03,\n",
      "         2.3102e-02, -2.9292e-02, -2.7600e-02, -1.3598e-02, -4.7868e-03,\n",
      "        -2.9981e-02, -4.2478e-02,  1.8418e-03, -1.7856e-02,  1.0439e-02,\n",
      "        -2.3290e-02, -4.6288e-02,  2.6730e-03, -6.2328e-02, -2.8965e-02,\n",
      "        -7.0481e-03,  1.0885e-02, -1.5038e-02, -4.6228e-02, -2.4972e-02,\n",
      "        -4.3593e-03, -2.8690e-02, -6.9849e-02, -2.3566e-03, -3.7714e-02,\n",
      "        -3.0319e-02, -3.7393e-02, -4.0120e-02, -3.3497e-02, -4.6720e-02,\n",
      "         1.7158e-03, -5.9526e-02, -3.4811e-02, -4.0534e-02, -3.0744e-03,\n",
      "        -7.0446e-02, -6.9497e-02,  2.5249e-02,  8.5411e-03, -1.2458e-02,\n",
      "        -1.2758e-02, -5.0639e-02, -2.5725e-02, -5.1508e-02, -1.1515e-02,\n",
      "        -5.1996e-02, -2.8115e-02, -1.9744e-02, -4.5064e-02,  4.6283e-03,\n",
      "        -5.3806e-02,  9.2340e-03, -6.8990e-02, -4.9764e-02, -4.6391e-02,\n",
      "        -4.7163e-02, -4.3103e-02, -2.0675e-02, -1.6390e-02, -9.2507e-03,\n",
      "        -4.2156e-02, -5.5544e-02,  2.8448e-02, -6.1811e-02, -6.8829e-02,\n",
      "        -3.7939e-02, -8.4251e-02, -1.3730e-02,  5.3402e-03, -2.0570e-02,\n",
      "        -8.8602e-02,  7.3412e-03, -3.2538e-02, -1.7643e-02, -2.9385e-02,\n",
      "        -3.9863e-02, -5.1289e-02, -6.9502e-02, -6.2700e-02,  1.3265e-02,\n",
      "        -9.7678e-03, -6.1701e-03, -2.1795e-03, -1.9694e-02,  1.4791e-03,\n",
      "        -1.1196e-03, -4.4842e-02, -2.0043e-02, -5.9127e-02, -8.0651e-03,\n",
      "        -1.7046e-03, -4.3695e-02, -4.8375e-02, -2.2870e-02, -4.7651e-02,\n",
      "        -5.6270e-02,  3.2454e-04, -3.3949e-02, -5.6396e-02, -6.5928e-03,\n",
      "        -2.3846e-02,  1.8305e-02, -1.8077e-02, -4.4872e-02, -2.3073e-02,\n",
      "        -6.0886e-02, -3.0661e-02, -1.1846e-02, -4.7783e-02, -3.7738e-02,\n",
      "        -1.4714e-02, -3.0778e-02, -1.7400e-02, -5.6706e-02, -3.0921e-02,\n",
      "         8.3397e-03, -4.1543e-02, -2.4540e-02,  8.4653e-03, -3.3943e-02,\n",
      "        -4.5272e-02, -1.9667e-02, -1.8907e-02, -1.9990e-02, -1.0903e-02,\n",
      "        -2.3400e-02, -8.9333e-02, -2.0799e-02, -4.9946e-02, -3.4581e-02,\n",
      "        -4.6051e-02,  2.1851e-03,  1.8178e-02, -1.1726e-02, -4.8570e-02,\n",
      "         1.8349e-02, -4.6886e-02, -3.6739e-02, -1.0548e-02,  1.9896e-02,\n",
      "         1.7099e-02, -3.0105e-02,  8.8589e-04, -4.1702e-02, -3.7705e-02,\n",
      "        -4.4662e-02, -1.5788e-02,  5.3412e-03, -3.9886e-02, -4.4785e-02,\n",
      "        -4.7005e-02, -2.5293e-03, -2.4526e-03, -1.7140e-03,  1.1435e-02,\n",
      "         9.5531e-03, -3.8266e-02, -4.8648e-02, -6.6560e-02, -1.6709e-02,\n",
      "        -3.6160e-02,  1.4954e-02, -4.7089e-02, -1.1903e-02, -3.9171e-03,\n",
      "        -5.8194e-02, -2.9812e-02, -4.1049e-02, -3.5342e-02, -7.2451e-02,\n",
      "        -1.4024e-02, -6.0485e-02,  1.2251e-02, -2.1388e-03, -7.5987e-02,\n",
      "        -5.8142e-03,  1.4463e-02, -5.0030e-02, -4.1097e-02,  2.0875e-02,\n",
      "        -2.9221e-02, -7.0062e-03, -4.2320e-02,  6.9784e-03, -7.3814e-03,\n",
      "        -5.9404e-02, -6.3004e-02, -1.3752e-02, -5.9353e-03, -5.5639e-02,\n",
      "        -4.2960e-02,  1.4043e-02, -4.3797e-02,  5.3324e-03, -4.6757e-02,\n",
      "        -6.8285e-02,  1.1510e-03, -4.8958e-02, -3.7783e-02, -4.4349e-02,\n",
      "        -4.3760e-02, -4.2985e-03, -3.5340e-02, -4.4789e-03, -5.8910e-02,\n",
      "        -3.8362e-02, -2.4943e-02, -5.9054e-02, -5.5209e-02, -3.5481e-02,\n",
      "        -4.5372e-02, -2.5521e-02, -5.8473e-02, -9.4464e-03,  2.9056e-03,\n",
      "         9.7785e-03,  1.2074e-02, -6.3030e-02, -5.8459e-02, -1.5188e-02,\n",
      "        -2.5142e-02,  1.3678e-02,  2.2829e-04, -6.7486e-03, -4.1130e-03,\n",
      "         1.2970e-02, -3.3620e-02, -5.8474e-02, -5.0279e-02,  3.8801e-02,\n",
      "         2.5239e-03, -5.7245e-03, -2.1381e-02, -3.0661e-02,  1.1686e-02,\n",
      "         1.7512e-02, -1.0994e-02, -1.5440e-02, -1.7209e-02,  2.8607e-03,\n",
      "         6.9849e-03, -3.0160e-02, -6.7839e-02, -3.3133e-02, -2.3626e-03,\n",
      "        -1.5495e-02, -1.5500e-02, -1.3970e-02, -6.4817e-02, -5.8142e-03,\n",
      "         8.1351e-03, -3.2296e-02, -9.7759e-03, -1.7915e-02, -5.4751e-02,\n",
      "         2.5673e-03, -6.6455e-02, -6.2897e-02,  5.5290e-03, -2.5790e-02,\n",
      "        -2.5374e-02, -3.3061e-02, -9.1721e-02, -3.9329e-02,  2.3568e-02,\n",
      "         8.8248e-03, -5.2595e-02, -6.6918e-02,  9.8367e-03, -4.5997e-02,\n",
      "        -7.1587e-02, -2.9349e-02, -2.9103e-03, -2.6324e-02, -1.6474e-02,\n",
      "        -4.8269e-02, -2.7229e-02,  2.1158e-02, -3.9943e-02, -9.4507e-04,\n",
      "        -2.1226e-02, -1.0665e-02, -3.6797e-03, -9.0415e-03, -6.9852e-02,\n",
      "        -3.3163e-02, -4.3905e-02, -4.9199e-02, -9.9182e-03, -2.1701e-03,\n",
      "        -6.4785e-02, -2.9009e-03,  2.4192e-03,  1.4460e-02,  1.4087e-02,\n",
      "        -3.3877e-02, -3.7477e-02,  7.2015e-03, -4.1175e-02, -3.5344e-02,\n",
      "        -7.0102e-02, -5.6945e-02,  1.0919e-02, -1.3172e-02,  8.6036e-03,\n",
      "        -1.0597e-02, -3.4291e-02, -3.4961e-02, -6.4875e-02, -2.8843e-02,\n",
      "        -6.9486e-02, -3.4750e-02, -1.8860e-02, -1.9982e-02, -3.6467e-02,\n",
      "        -7.7164e-03, -3.7372e-03, -2.6471e-02, -1.5754e-02, -1.0007e-02,\n",
      "        -3.4370e-02, -4.6951e-02, -1.9288e-02, -2.7411e-02, -4.1405e-02,\n",
      "        -3.8060e-02, -3.2429e-02, -4.9567e-02, -3.3999e-02,  1.5264e-02,\n",
      "        -5.5164e-02,  1.7429e-02, -5.2438e-02, -6.6351e-02, -9.5689e-03,\n",
      "        -1.6869e-02, -4.0993e-02, -3.8860e-02, -3.1027e-02, -1.4952e-02,\n",
      "        -2.2987e-02, -3.6939e-02, -2.9376e-02, -3.2377e-02, -4.1908e-02,\n",
      "        -2.1108e-02, -3.0083e-02, -4.8110e-03, -3.4153e-02, -1.8659e-02,\n",
      "        -1.9309e-02, -3.5151e-02, -1.9881e-02, -1.4530e-02,  1.2368e-02,\n",
      "         1.7925e-02, -1.3605e-02, -2.2076e-02, -7.5903e-02,  5.4306e-05,\n",
      "         1.1935e-06, -2.7873e-02, -1.0017e-02, -5.5257e-02, -4.8782e-02,\n",
      "        -2.1021e-02, -3.8979e-02, -5.3838e-06, -4.9756e-02, -3.8368e-02,\n",
      "        -2.5686e-02, -6.7939e-02, -1.7220e-02, -4.2260e-02, -6.8785e-02,\n",
      "         2.1230e-02, -3.1740e-02, -3.4863e-02, -9.9920e-02, -3.9954e-02,\n",
      "        -2.7520e-02, -2.8407e-02, -2.7549e-02, -4.1503e-02, -2.3085e-02,\n",
      "        -3.6853e-02, -1.0950e-02, -8.3924e-03,  4.3932e-03, -3.1146e-02,\n",
      "        -3.2386e-02, -1.5412e-02, -2.4588e-02, -1.0638e-02, -1.9955e-02,\n",
      "         2.0616e-02,  2.0261e-02,  4.5436e-02, -4.4676e-03,  3.7797e-03,\n",
      "        -3.9898e-02, -7.5682e-02, -1.2726e-01, -5.8453e-02, -3.4450e-02,\n",
      "         1.2098e-03, -1.5100e-02, -6.9339e-02, -8.4884e-02, -8.5773e-03,\n",
      "        -7.5924e-02, -6.2510e-02, -6.6021e-03, -3.4028e-02, -3.0930e-02,\n",
      "        -3.9283e-02, -3.0349e-02, -3.2138e-02,  1.0340e-02, -4.0021e-02,\n",
      "        -3.5385e-02, -8.4299e-02, -4.9472e-02, -5.3962e-02, -2.6773e-02,\n",
      "         1.7104e-02,  4.4100e-03,  1.2650e-03, -8.4897e-02, -4.5148e-02,\n",
      "        -4.2534e-02, -3.6277e-02, -2.9447e-02, -4.5347e-02, -1.8768e-02,\n",
      "         1.1657e-03, -3.0215e-02,  1.7721e-02, -3.2083e-02,  9.7653e-04,\n",
      "        -7.1313e-02, -1.3837e-02, -3.5460e-02, -6.6297e-02, -4.7993e-02,\n",
      "        -4.6755e-02, -3.7162e-03, -4.2796e-02, -6.2296e-02, -2.7312e-02,\n",
      "        -5.3962e-02, -2.1160e-02, -2.6156e-02, -3.2681e-02, -3.4077e-02,\n",
      "        -5.7537e-02, -4.4260e-02, -1.3634e-02,  1.6239e-04, -7.7819e-02,\n",
      "        -6.6047e-02, -2.8107e-02, -4.8476e-02, -5.8603e-02, -3.7061e-02],\n",
      "       device='cuda:0'), 'linear2.weight': tensor([[ 7.9093e-02,  2.8553e-02, -7.9211e-03,  ...,  2.2454e-02,\n",
      "          8.5938e-03, -3.7821e-05],\n",
      "        [-1.9746e-02,  2.2571e-02,  4.3749e-03,  ..., -4.2319e-02,\n",
      "          2.1650e-02,  2.7708e-02],\n",
      "        [ 2.5250e-03,  4.1303e-03,  2.1624e-02,  ..., -4.1879e-03,\n",
      "          3.2229e-03, -3.0218e-02],\n",
      "        ...,\n",
      "        [-6.1786e-03,  2.2509e-02,  8.6153e-03,  ..., -5.5996e-02,\n",
      "          5.3255e-03,  1.7324e-02],\n",
      "        [ 9.8761e-03, -3.1048e-02, -2.1820e-02,  ...,  2.6545e-02,\n",
      "          1.9154e-02,  2.4815e-02],\n",
      "        [-3.0696e-02, -2.1658e-02, -8.3940e-03,  ..., -4.3122e-03,\n",
      "          1.1412e-02, -1.2213e-02]], device='cuda:0'), 'linear2.bias': tensor([-0.0348, -0.0488, -0.1296,  ..., -0.0291, -0.0256, -0.0845],\n",
      "       device='cuda:0'), 'linear3.weight': tensor([[ 0.0141, -0.0055,  0.0055,  ...,  0.0458,  0.0159,  0.0082],\n",
      "        [-0.1492, -0.0098, -0.0042,  ..., -0.1219, -0.0247, -0.0360],\n",
      "        [-0.0095, -0.0147, -0.0218,  ..., -0.0555, -0.0171, -0.0097],\n",
      "        ...,\n",
      "        [-0.0378,  0.0115,  0.0197,  ..., -0.1878, -0.0113, -0.0422],\n",
      "        [-0.0296, -0.0818, -0.0445,  ..., -0.1449, -0.0061,  0.0216],\n",
      "        [ 0.0506, -0.0373,  0.0017,  ..., -0.0197, -0.0078,  0.0092]],\n",
      "       device='cuda:0'), 'linear3.bias': tensor([ 0.0160, -0.1075, -0.0298, -0.0572, -0.0096, -0.0472, -0.0454, -0.0873,\n",
      "         0.2089,  0.0468], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# copy of the original model:\n",
    "original_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()\n",
    "print(original_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 292.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.962\n",
      "Number 0 was wrong 16 times\n",
      "Number 1 was wrong 19 times\n",
      "Number 2 was wrong 27 times\n",
      "Number 3 was wrong 53 times\n",
      "Number 4 was wrong 67 times\n",
      "Number 5 was wrong 24 times\n",
      "Number 6 was wrong 20 times\n",
      "Number 7 was wrong 53 times\n",
      "Number 8 was wrong 50 times\n",
      "Number 9 was wrong 51 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the performance of the trained model:\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_data_loader, desc=\"Testing\"):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 28*28))\n",
    "            \n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] += 1\n",
    "                total += 1\n",
    "    \n",
    "    print(f\"Accuracy: {round(correct/total, 3)}\")\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f\"Number {i} was wrong {wrong_counts[i]} times\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 has torch.Size([1000, 784]) weights and torch.Size([1000]) biases\n",
      "Layer 2 has torch.Size([2000, 1000]) weights and torch.Size([2000]) biases\n",
      "Layer 3 has torch.Size([10, 2000]) weights and torch.Size([10]) biases\n",
      "Total number of parameters in the original model: 2807010\n"
     ]
    }
   ],
   "source": [
    "# inspecting the total number of parameters:\n",
    "total_params_original = 0\n",
    "for index, layer in enumerate([model.linear1, model.linear2, model.linear3]):\n",
    "    total_params_original += layer.weight.numel() + layer.bias.numel()\n",
    "    print(f\"Layer {index+1} has {layer.weight.shape} weights and {layer.bias.shape} biases\")\n",
    "print(f\"Total number of parameters in the original model: {total_params_original}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the LoRA parametrization:https://github.com/hkproj/pytorch-lora/blob/main/lora.ipynb\n",
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        # Section 4.1 of the paper:\n",
    "        # We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank, features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "\n",
    "        # Section 4.1 of the paper:\n",
    "        # We then scale ∆Wx by α/r , where α is a constant in r.\n",
    "        # When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately.\n",
    "        # As a result, we simply set α to the first r we try and do not tune it.\n",
    "        # This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # Return W + (B*A)*scale\n",
    "            return (\n",
    "                original_weights\n",
    "                + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape)\n",
    "                * self.scale\n",
    "            )\n",
    "        else:\n",
    "            return original_weights\n",
    "\n",
    "\n",
    "# applying the LoRA parametrization to the model:\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
    "    # From section 4.2 of the paper:\n",
    "    # We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules\n",
    "    # (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n",
    "    # We leave the empirical investigation of [...], and biases to a future work.\n",
    "\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear1, \"weight\", linear_layer_parameterization(model.linear1, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear2, \"weight\", linear_layer_parameterization(model.linear2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear3, \"weight\", linear_layer_parameterization(model.linear3, device)\n",
    ")\n",
    "\n",
    "\n",
    "# function to enable/disable the LoRA parametrization:\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [model.linear1, model.linear2, model.linear3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 has torch.Size([1, 784]) LoRA parameters and torch.Size([1000, 784]) non-LoRA parameters and torch.Size([1000]) biases and torch.Size([1000, 1]) LoRA parameters\n",
      "Layer 2 has torch.Size([1, 1000]) LoRA parameters and torch.Size([2000, 1000]) non-LoRA parameters and torch.Size([2000]) biases and torch.Size([2000, 1]) LoRA parameters\n",
      "Layer 3 has torch.Size([1, 2000]) LoRA parameters and torch.Size([10, 2000]) non-LoRA parameters and torch.Size([10]) biases and torch.Size([10, 1]) LoRA parameters\n",
      "Total number of parameters in original model: 2807010\n",
      "Total number of parameters in LoRA model: 6794\n",
      "Total number of parameters in original + LoRA model: 2813804\n",
      "Parameters increased by 0.242%\n"
     ]
    }
   ],
   "source": [
    "# inspecting the total number of parameters after applying the LoRA parametrization:\n",
    "total_params_lora = 0\n",
    "total_params_non_lora = 0\n",
    "for index, layer in enumerate([model.linear1, model.linear2, model.linear3]):\n",
    "    total_params_lora += (\n",
    "        layer.parametrizations[\"weight\"][0].lora_A.numel()\n",
    "        + layer.parametrizations[\"weight\"][0].lora_B.numel()\n",
    "    )\n",
    "    total_params_non_lora += layer.weight.numel() + layer.bias.numel()\n",
    "    print(\n",
    "        f\"Layer {index+1} has {layer.parametrizations['weight'][0].lora_A.shape} LoRA parameters and {layer.weight.shape} non-LoRA parameters and {layer.bias.shape} biases and {layer.parametrizations['weight'][0].lora_B.shape} LoRA parameters\"\n",
    "    )\n",
    "\n",
    "assert total_params_non_lora == total_params_original\n",
    "print(f\"Total number of parameters in original model: {total_params_non_lora}\")\n",
    "print(f\"Total number of parameters in LoRA model: {total_params_lora}\")\n",
    "print(\n",
    "    f\"Total number of parameters in original + LoRA model: {total_params_non_lora + total_params_lora}\"\n",
    ")\n",
    "parameters_increased = (total_params_lora / total_params_non_lora) * 100\n",
    "print(f\"Parameters increased by {round(parameters_increased, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 173.50it/s, loss=0.121]\n"
     ]
    }
   ],
   "source": [
    "# freeze the non-LoRA parameters:\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        print(f\"Freezing non-LoRA parameter {name}\")\n",
    "        param.requires_grad = False\n",
    "\n",
    "# load the mnist dataset again keeping only the digit 4:\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "include_indices = mnist_trainset.targets == 4\n",
    "mnist_trainset.data = mnist_trainset.data[include_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[include_indices]\n",
    "train_data_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# train the network for 1 epoch for only 100 batches:\n",
    "train(train_data_loader, model, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that the frozen parameters are still unchanged by the finetuning\n",
    "assert torch.all(\n",
    "    model.linear1.parametrizations.weight.original == original_weights[\"linear1.weight\"]\n",
    ")\n",
    "assert torch.all(\n",
    "    model.linear2.parametrizations.weight.original == original_weights[\"linear2.weight\"]\n",
    ")\n",
    "assert torch.all(\n",
    "    model.linear3.parametrizations.weight.original == original_weights[\"linear3.weight\"]\n",
    ")\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n",
    "# The original weights have been moved to net.linear1.parametrizations.weight.original\n",
    "# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
    "assert torch.equal(\n",
    "    model.linear1.weight,\n",
    "    model.linear1.parametrizations.weight.original\n",
    "    + (\n",
    "        model.linear1.parametrizations.weight[0].lora_B\n",
    "        @ model.linear1.parametrizations.weight[0].lora_A\n",
    "    )\n",
    "    * model.linear1.parametrizations.weight[0].scale,\n",
    ")\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "assert torch.equal(model.linear1.weight, original_weights[\"linear1.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 266.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.949\n",
      "Number 0 was wrong 17 times\n",
      "Number 1 was wrong 22 times\n",
      "Number 2 was wrong 37 times\n",
      "Number 3 was wrong 67 times\n",
      "Number 4 was wrong 16 times\n",
      "Number 5 was wrong 31 times\n",
      "Number 6 was wrong 20 times\n",
      "Number 7 was wrong 58 times\n",
      "Number 8 was wrong 73 times\n",
      "Number 9 was wrong 174 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test when the LoRA parametrization is enabled:\n",
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 287.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.962\n",
      "Number 0 was wrong 16 times\n",
      "Number 1 was wrong 19 times\n",
      "Number 2 was wrong 27 times\n",
      "Number 3 was wrong 53 times\n",
      "Number 4 was wrong 67 times\n",
      "Number 5 was wrong 24 times\n",
      "Number 6 was wrong 20 times\n",
      "Number 7 was wrong 53 times\n",
      "Number 8 was wrong 50 times\n",
      "Number 9 was wrong 51 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test when the LoRA parametrization is disabled:\n",
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
